[{"id":"470f1f21-24bd-4278-96a0-2f4673918583","userId":"9baf9615-b716-4ab3-bc0f-67500dcd4210","tool":{"id":"web_search","name":"Web Search","meta":{"description":"Web Search using SearXNG and Scrap first N Pages","manifest":{"title":"Web Search using SearXNG and Scrape first N Pages","author":"constLiakos with enhancements by justinh-rahb and ther3zz","funding_url":"https://github.com/EntropyYue/web_search","version":"0.4.4","license":"MIT"}},"content":"\"\"\"\ntitle: Web Search using SearXNG and Scrape first N Pages\nauthor: constLiakos with enhancements by justinh-rahb and ther3zz\nfunding_url: https://github.com/EntropyYue/web_search\nversion: 0.4.4\nlicense: MIT\n\"\"\"\n\nimport requests\nimport json\nfrom bs4 import BeautifulSoup\nimport concurrent.futures\nfrom urllib.parse import urlparse\nimport re\nimport unicodedata\nfrom pydantic import BaseModel, Field\nfrom typing import Callable, Any\n\n\nclass HelpFunctions:\n    def __init__(self):\n        pass\n\n    def get_base_url(self, url):\n        parsed_url = urlparse(url)\n        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n        return base_url\n\n    def generate_excerpt(self, content, max_length=200):\n        return content[:max_length] + \"...\" if len(content) > max_length else content\n\n    def format_text(self, original_text, valves):\n        soup = BeautifulSoup(original_text, \"html.parser\")\n        formatted_text = soup.get_text(separator=\" \", strip=True)\n        formatted_text = unicodedata.normalize(\"NFKC\", formatted_text)\n        formatted_text = re.sub(r\"\\s+\", \" \", formatted_text)\n        formatted_text = formatted_text.strip()\n        formatted_text = self.remove_emojis(formatted_text)\n        if valves.REMOVE_LINKS:\n            formatted_text = self.replace_urls_with_text(formatted_text)\n        return formatted_text\n\n    def remove_emojis(self, text):\n        return \"\".join(c for c in text if not unicodedata.category(c).startswith(\"So\"))\n\n    def replace_urls_with_text(self, text, replacement=\"(links)\"):\n        pattern = r\"\\(https?://[^\\s]+\\)\"\n        return re.sub(pattern, replacement, text)\n\n    def process_search_result(self, result, valves):\n        title_site = self.remove_emojis(result[\"title\"])\n        url_site = result[\"url\"]\n        snippet = result.get(\"content\", \"\")\n\n        # Check if the website is in the ignored list, but only if IGNORED_WEBSITES is not empty\n        if valves.IGNORED_WEBSITES:\n            base_url = self.get_base_url(url_site)\n            if any(\n                ignored_site.strip() in base_url\n                for ignored_site in valves.IGNORED_WEBSITES.split(\",\")\n            ):\n                return None\n\n        try:\n            response_site = requests.get(\n                valves.JINA_READER_BASE_URL + url_site, timeout=20\n            )\n            response_site.raise_for_status()\n            html_content = response_site.text\n\n            soup = BeautifulSoup(html_content, \"html.parser\")\n            content_site = self.format_text(\n                soup.get_text(separator=\" \", strip=True), valves\n            )\n\n            truncated_content = self.truncate_to_n_words(\n                content_site, valves.PAGE_CONTENT_WORDS_LIMIT\n            )\n\n            return {\n                \"title\": title_site,\n                \"url\": url_site,\n                \"content\": truncated_content,\n                \"snippet\": self.remove_emojis(snippet),\n            }\n\n        except requests.exceptions.RequestException:\n            return None\n\n    def truncate_to_n_words(self, text, token_limit):\n        tokens = text.split()\n        truncated_tokens = tokens[:token_limit]\n        return \" \".join(truncated_tokens)\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def emit(\n        self,\n        description=\"未知状态\",\n        status=\"in_progress\",\n        done=False,\n        action=\"\",\n        urls=[],\n    ):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                        \"action\": action,\n                        \"urls\": urls,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        SEARXNG_ENGINE_API_BASE_URL: str = Field(\n            default=\"https://example.com/search\",\n            description=\"搜索引擎的基础URL\",\n        )\n        IGNORED_WEBSITES: str = Field(\n            default=\"\",\n            description=\"以逗号分隔的要忽略的网站列表\",\n        )\n        RETURNED_SCRAPPED_PAGES_NO: int = Field(\n            default=3,\n            description=\"要分析的搜索引擎结果数\",\n        )\n        SCRAPPED_PAGES_NO: int = Field(\n            default=5,\n            description=\"已分页的总页数。理想情况下，大于返回的页面之一\",\n        )\n        PAGE_CONTENT_WORDS_LIMIT: int = Field(\n            default=5000,\n            description=\"限制每页的字数\",\n        )\n        CITATION_LINKS: bool = Field(\n            default=False,\n            description=\"如果为True，则发送带有链接的自定义引用\",\n        )\n        JINA_READER_BASE_URL: str = Field(\n            default=\"https://r.jina.ai/\",\n            description=\"Jina Reader的基础URL\",\n        )\n        REMOVE_LINKS: bool = Field(\n            default=True,\n            description=\"检索中的返回是否移除链接\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n\n    async def search_web(\n        self,\n        query: str,\n        __event_emitter__: Callable[[dict], Any] = None,\n    ) -> str:\n        \"\"\"\n        搜索网络并获取相关页面的内容，搜索未知知识、新闻、信息、公共联系信息、天气等\n\n        :params query: 搜索中使用的关键词\n\n        :return: The content of the pages in json format.\n        \"\"\"\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.emit(f\"正在搜索: {query}\")\n\n        search_engine_url = self.valves.SEARXNG_ENGINE_API_BASE_URL\n\n        # Ensure RETURNED_SCRAPPED_PAGES_NO does not exceed SCRAPPED_PAGES_NO\n        if self.valves.RETURNED_SCRAPPED_PAGES_NO > self.valves.SCRAPPED_PAGES_NO:\n            self.valves.RETURNED_SCRAPPED_PAGES_NO = self.valves.SCRAPPED_PAGES_NO\n\n        params = {\n            \"q\": query,\n            \"format\": \"json\",\n            \"number_of_results\": self.valves.RETURNED_SCRAPPED_PAGES_NO,\n        }\n\n        try:\n            await emitter.emit(\"正在向搜索引擎发送请求\")\n            resp = requests.get(\n                search_engine_url, params=params, headers=self.headers, timeout=120\n            )\n            resp.raise_for_status()\n            data = resp.json()\n\n            results = data.get(\"results\", [])\n            limited_results = results[: self.valves.SCRAPPED_PAGES_NO]\n            await emitter.emit(f\"返回了 {len(limited_results)} 个搜索结果\")\n\n        except requests.exceptions.RequestException as e:\n            await emitter.emit(\n                status=\"error\",\n                description=f\"搜索时出错: {str(e)}\",\n                done=True,\n            )\n            return json.dumps({\"error\": str(e)})\n\n        results_json = []\n        if limited_results:\n            await emitter.emit(\"正在处理搜索结果\")\n\n            try:\n                with concurrent.futures.ThreadPoolExecutor() as executor:\n                    futures = [\n                        executor.submit(\n                            functions.process_search_result, result, self.valves\n                        )\n                        for result in limited_results\n                    ]\n\n                    processed_count = 0\n                    for future in concurrent.futures.as_completed(futures):\n                        result_json = future.result()\n                        if result_json:\n                            try:\n                                results_json.append(result_json)\n                                processed_count += 1\n                                await emitter.emit(\n                                    f\"处理页面 {processed_count}/{len(limited_results)}\",\n                                )\n                            except (TypeError, ValueError, Exception) as e:\n                                print(f\"处理时出错: {str(e)}\")\n                                continue\n                        if len(results_json) >= self.valves.RETURNED_SCRAPPED_PAGES_NO:\n                            break\n\n            except BaseException as e:\n                await emitter.emit(\n                    status=\"error\",\n                    description=f\"处理时出错: {str(e)}\",\n                    done=True,\n                )\n\n            results_json = results_json[: self.valves.RETURNED_SCRAPPED_PAGES_NO]\n\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                if len(results_json):\n                    for result in results_json:\n                        await __event_emitter__(\n                            {\n                                \"type\": \"citation\",\n                                \"data\": {\n                                    \"document\": [result[\"content\"]],\n                                    \"metadata\": [{\"source\": result[\"url\"]}],\n                                    \"source\": {\"name\": result[\"title\"]},\n                                },\n                            }\n                        )\n\n        urls = []\n        for result in results_json:\n            urls.append(result[\"url\"])\n\n        await emitter.emit(\n            status=\"complete\",\n            description=f\"网络搜索已完成,将从 {len(results_json)} 个页面检索内容\",\n            done=True,\n            action=\"web_search\",\n            urls=urls,\n        )\n\n        return json.dumps(results_json, indent=4, ensure_ascii=False)\n\n    async def get_website(\n        self, url: str, __event_emitter__: Callable[[dict], Any] = None\n    ) -> str:\n        \"\"\"\n        打开输入的网站并获取其内容\n\n        :params url: 需要打开的网站\n\n        :return: The content of the website in json format.\n        \"\"\"\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.emit(f\"正在从URL获取内容: {url}\")\n\n        results_json = []\n\n        try:\n            response_site = requests.get(\n                self.valves.JINA_READER_BASE_URL + url,\n                headers=self.headers,\n                timeout=120,\n            )\n            response_site.raise_for_status()\n            html_content = response_site.text\n\n            soup = BeautifulSoup(html_content, \"html.parser\")\n\n            page_title = soup.title.string if soup.title else \"No title found\"\n            page_title = unicodedata.normalize(\"NFKC\", page_title.strip())\n            page_title = functions.remove_emojis(page_title)\n            title_site = page_title\n            url_site = url\n            content_site = functions.format_text(\n                soup.get_text(separator=\" \", strip=True), self.valves\n            )\n\n            truncated_content = functions.truncate_to_n_words(\n                content_site, self.valves.PAGE_CONTENT_WORDS_LIMIT\n            )\n\n            result_site = {\n                \"title\": title_site,\n                \"url\": url_site,\n                \"content\": truncated_content,\n                \"excerpt\": functions.generate_excerpt(content_site),\n            }\n\n            results_json.append(result_site)\n\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"citation\",\n                        \"data\": {\n                            \"document\": [truncated_content],\n                            \"metadata\": [{\"source\": url_site}],\n                            \"source\": {\"name\": title_site},\n                        },\n                    }\n                )\n\n            await emitter.emit(\n                status=\"complete\",\n                description=\"已成功检索和处理网站内容\",\n                done=True,\n            )\n\n        except requests.exceptions.RequestException as e:\n            results_json.append(\n                {\n                    \"url\": url,\n                    \"content\": f\"检索页面失败,错误: {str(e)}\",\n                }\n            )\n\n            await emitter.emit(\n                status=\"error\",\n                description=f\"获取网站内容时出错: {str(e)}\",\n                done=True,\n            )\n\n        return json.dumps(results_json, indent=4, ensure_ascii=False)"},"downloads":1444,"upvotes":0,"downvotes":0,"updatedAt":1730092647,"createdAt":1730092647,"user":{"id":"9baf9615-b716-4ab3-bc0f-67500dcd4210","username":"xiaopa233","name":"","profileImageUrl":"https://www.gravatar.com/avatar/ce43a2172b4d2005a773c3cc5741b6da4a9ed9945e7f4bc254e06359e72558d6?d=mp","createdAt":1716317794}}]