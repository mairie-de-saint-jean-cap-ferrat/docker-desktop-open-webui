[{"id":"fe8783ec-2f46-444c-b925-01ba62c3c4fd","userId":"a84a6d10-60f2-470a-a011-b5f33bb0cc15","tool":{"id":"research","name":"Research","meta":{"description":"Research using SRNXG with BeautifulSoup (Optimized Deep Research)","manifest":{"title":"Web Search using SRNXG with BeautifulSoup (Optimized Deep Research)","author":"Teodor Cucu (Credits to Jacob DeLacerda for giving me the Idea)","version":"0.2.3","github":"https://github.com/the-real-t30d0r/research-openwebui","license":"MIT"}},"content":"\"\"\"\ntitle:Web Search using SRNXG with BeautifulSoup (Optimized Deep Research)\nauthor: Teodor Cucu (Credits to Jacob DeLacerda for giving me the Idea)\nversion: 0.2.3\ngithub: https://github.com/the-real-t30d0r/research-openwebui\nlicense: MIT\n\n\nYOU NEED THIS PROMPT IN ORDER TO USE IT: https://openwebui.com/p/t30d0r99/research\n\"\"\"\n\n#!/usr/bin/env python3\nimport os\nimport json\nfrom urllib.parse import urlparse\nimport re\nimport unicodedata\nfrom pydantic import BaseModel, Field\nimport asyncio\nimport aiohttp\nfrom typing import Any, Callable\nfrom bs4 import BeautifulSoup\n\n\nclass HelpFunctions:\n    def get_base_url(self, url: str) -> str:\n        url_components = urlparse(url)\n        return f\"{url_components.scheme}://{url_components.netloc}\"\n\n    def generate_excerpt(self, content: str, max_length: int = 200) -> str:\n        return content[:max_length] + \"...\" if len(content) > max_length else content\n\n    def format_text(self, text: str) -> str:\n        text = unicodedata.normalize(\"NFKC\", text)\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text.strip()\n\n    def remove_emojis(self, text: str) -> str:\n        return \"\".join(c for c in text if not unicodedata.category(c).startswith(\"So\"))\n\n    def truncate_to_n_words(self, text: str, n: int) -> str:\n        words = text.split()\n        return \" \".join(words[:n])\n\n    async def fallback_scrape_async(\n        self, url_site: str, timeout: int = 20, retries: int = 3\n    ) -> Any:\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n        }\n        for attempt in range(retries):\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.get(\n                        url_site, headers=headers, timeout=timeout\n                    ) as response:\n                        response.raise_for_status()\n                        html_content = await response.text()\n                soup = BeautifulSoup(html_content, \"html.parser\")\n                for script in soup([\"script\", \"style\"]):\n                    script.extract()\n                text = soup.get_text(separator=\" \")\n                formatted_text = self.format_text(text)\n                if len(formatted_text) < 50:\n                    raise ValueError(\"Content too short\")\n                return formatted_text\n            except Exception as e:\n                await asyncio.sleep(1)\n        return None\n\n    async def process_search_result(self, result: dict, valves: Any) -> Any:\n        url_site = result.get(\"url\", \"\")\n        if not url_site:\n            return None\n        if valves.IGNORED_WEBSITES:\n            base_url = self.get_base_url(url_site)\n            if any(\n                ignored.strip() in base_url\n                for ignored in valves.IGNORED_WEBSITES.split(\",\")\n            ):\n                return None\n        content = await self.fallback_scrape_async(url_site)\n        if not content:\n            content = result.get(\"content\", \"\")\n        if not content or len(content) < 50:\n            return None\n        return {\n            \"title\": self.remove_emojis(result.get(\"title\", \"\")),\n            \"url\": url_site,\n            \"content\": self.truncate_to_n_words(\n                content, valves.PAGE_CONTENT_WORDS_LIMIT\n            ),\n            \"snippet\": self.remove_emojis(result.get(\"content\", \"\")),\n        }\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        SRNXG_API_BASE_URL: str = Field(\n            default=\"http://0.0.0.0:9090\", description=\"Local SearXNG API base URL\"\n        )\n        IGNORED_WEBSITES: str = Field(\n            default=\"\", description=\"Comma-separated list of websites to ignore\"\n        )\n        TOTAL_PAGES_COUNT: int = Field(\n            default=100, description=\"Number of pages to search per query\"\n        )\n        RETURNED_PAGES_COUNT: int = Field(\n            default=100, description=\"Number of pages to return\"\n        )\n        PAGE_CONTENT_WORDS_LIMIT: int = Field(\n            default=6000, description=\"Word limit per page for context\"\n        )\n        CITATION_LINKS: bool = Field(\n            default=True, description=\"Include citation metadata\"\n        )\n        MAX_ITERATIONS: int = Field(\n            default=5, description=\"Maximum iterations per query\"\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.headers = {\n            \"X-No-Cache\": \"true\",\n            \"X-With-Images-Summary\": \"true\",\n            \"X-With-Links-Summary\": \"true\",\n        }\n\n    def refine_query(self, topic: str, iteration: int) -> str:\n        refine_terms = [\n            \"detailed analysis\",\n            \"comprehensive review\",\n            \"in-depth insights\",\n            \"extended study\",\n            \"thorough investigation\",\n        ]\n        term = refine_terms[min(iteration, len(refine_terms) - 1)]\n        return f\"{topic} {term}\"\n\n    def generate_report(self, topic: str, results: list) -> str:\n        report = f\"# Deep Research Report on {topic}\\n\\n\"\n        report += \"## Mission Outcome and Planning\\n\"\n        report += (\n            \"A series of iterative internet searches were performed. For each source, key insights, adjustments to current findings, and missing information were noted. This process repeated until sufficient data was gathered or \"\n            + str(self.valves.MAX_ITERATIONS)\n            + \" iterations were reached.\\n\\n\"\n        )\n        if not results:\n            report += \"No relevant sources were found.\\n\"\n            return report\n        for idx, result in enumerate(results, start=1):\n            report += f\"### Source {idx}: {result['title']}\\n\"\n            report += f\"**URL:** {result['url']}\\n\\n\"\n            report += \"#### Key Insights\\n\"\n            report += f\"{result['content'][:300]}...\\n\\n\"\n            report += \"#### Adjustments to Current Findings\\n\"\n            report += \"To be determined based on further analysis.\\n\\n\"\n            report += \"#### Missing Information\\n\"\n            report += \"To be determined based on further analysis.\\n\\n\"\n        report += \"## Citations\\n\"\n        for idx, result in enumerate(results, start=1):\n            report += f\"{idx}. [{result['title']}]({result['url']})\\n\"\n        return report\n\n    async def search_web(\n        self, query: str, __event_emitter__: Callable[[dict], Any] = None\n    ) -> str:\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n        topic = query\n        search_query = query\n        await emitter.emit(\n            description=f\"Internet search initiated for: {topic}. Please wait...\",\n            status=\"in_progress\",\n            done=False,\n        )\n        all_results = []\n        seen_urls = set()\n        max_iterations = self.valves.MAX_ITERATIONS\n        for iteration in range(max_iterations):\n            offset = iteration * self.valves.TOTAL_PAGES_COUNT\n            await emitter.emit(\n                description=f\"Iteration {iteration+1}: Searching for '{search_query}' with offset {offset}...\",\n                status=\"in_progress\",\n                done=False,\n            )\n            params = {\n                \"q\": search_query,\n                \"format\": \"json\",\n                \"number_of_results\": self.valves.TOTAL_PAGES_COUNT,\n                \"offset\": offset,\n            }\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.get(\n                        f\"{self.valves.SRNXG_API_BASE_URL}/search\",\n                        params=params,\n                        headers=self.headers,\n                        timeout=120,\n                    ) as response:\n                        response.raise_for_status()\n                        json_data = await response.json()\n                search_items = json_data.get(\"results\", [])\n            except Exception as e:\n                await emitter.emit(\n                    description=f\"Search error: {str(e)}\", status=\"error\", done=True\n                )\n                break\n            if not search_items:\n                await emitter.emit(\n                    description=\"No more search results found. Ending search iterations.\",\n                    status=\"in_progress\",\n                    done=False,\n                )\n                break\n            new_results = []\n            tasks = [\n                functions.process_search_result(\n                    {\n                        \"title\": item.get(\"title\", \"\"),\n                        \"url\": item.get(\"url\", \"\"),\n                        \"content\": item.get(\"snippet\", \"\"),\n                    },\n                    self.valves,\n                )\n                for item in search_items\n            ]\n            processed_results = await asyncio.gather(*tasks)\n            for res in processed_results:\n                if res and res[\"url\"] not in seen_urls:\n                    seen_urls.add(res[\"url\"])\n                    new_results.append(res)\n            if not new_results:\n                break\n            all_results.extend(new_results)\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                for result in new_results:\n                    await __event_emitter__(\n                        {\n                            \"type\": \"citation\",\n                            \"data\": {\n                                \"document\": [result[\"content\"]],\n                                \"metadata\": [{\"source\": result[\"url\"]}],\n                                \"source\": {\"name\": result[\"title\"]},\n                            },\n                        }\n                    )\n            await emitter.emit(\n                description=f\"Iteration {iteration+1} added {len(new_results)} valid results (Total: {len(all_results)})\",\n                status=\"in_progress\",\n                done=False,\n            )\n            if len(all_results) >= self.valves.RETURNED_PAGES_COUNT:\n                break\n        if not all_results:\n            await emitter.emit(\n                description=\"Web search completed. No relevant sources were found.\",\n                status=\"complete\",\n                done=True,\n            )\n            report = self.generate_report(topic, [])\n        else:\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                citation_summary = [\n                    {\"title\": r[\"title\"], \"url\": r[\"url\"]} for r in all_results\n                ]\n                await __event_emitter__(\n                    {\n                        \"type\": \"citation_summary\",\n                        \"data\": {\n                            \"message\": f\"Visited {len(all_results)} websites.\",\n                            \"citations\": citation_summary,\n                        },\n                    }\n                )\n            await emitter.emit(\n                description=f\"Web search completed. Retrieved content from {len(all_results)} pages.\",\n                status=\"complete\",\n                done=True,\n            )\n            report = self.generate_report(topic, all_results)\n        return report\n\n\nasync def main():\n    async def my_event_handler(event: dict):\n        print(\n            f\"Event: {event['data']['status']} - {event['data']['description']} (Done: {event['data']['done']})\"\n        )\n\n    tools = Tools()\n    report = await tools.search_web(\"test\", __event_emitter__=my_event_handler)\n    print(\"\\n--- REPORT ---\\n\")\n    print(report)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"},"downloads":1011,"upvotes":0,"downvotes":0,"updatedAt":1739203435,"createdAt":1739187258,"user":{"id":"a84a6d10-60f2-470a-a011-b5f33bb0cc15","username":"t30d0r99","name":"","profileImageUrl":"https://www.gravatar.com/avatar/e5bfa8d2840debb2ac7fa37c91d8f181329341209d4bf8b9b17959c78576c63b?d=mp","createdAt":1738851606}}]